{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mashup of stuff from\n",
    "# https://github.com/leongatys/PytorchNeuralStyleTransfer/blob/master/NeuralStyleTransfer.ipynb\n",
    "# https://github.com/alexis-jacq/Pytorch-Tutorials/blob/master/Neural_Style.ipynb\n",
    "# https://arxiv.org/abs/1508.06576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsize = 512\n",
    "loader = transforms.Compose([\n",
    "    transforms.Scale((imsize, imsize)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    # need a fake batch dim for nn\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the images\n",
    "unloader = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "def imshow(tensor):\n",
    "    image = tensor.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.loss = self.criterion.forward(input * self.weight, self.target)\n",
    "        self.output = input\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size() # a = batch size, b = K (num maps), c * d = N (size of map)\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        # renormalize\n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram.forward(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion.forward(self.G, self.target)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(content, style, cnn, content_weight=1.0, style_weight=10000.0):\n",
    "    content_layers = [\"r42\"]\n",
    "    style_layers = [\"r11\", \"r21\", \"r31\", \"r41\", \"r51\"]\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential().cuda()\n",
    "    gram = GramMatrix().cuda()\n",
    "\n",
    "    i = 1\n",
    "    j = 0\n",
    "    for layer in list(cnn):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            j += 1\n",
    "            name = \"c{}{}\".format(i,j)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"r{}{}\".format(i, j)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                target = model.forward(content).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_{}_{}\".format(i, j), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                target_feature = model.forward(style).clone()\n",
    "                target_feature_gram = gram.forward(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_{}_{}\".format(i, j), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            name = \"p{}\".format(i)\n",
    "            avgpool = nn.AvgPool2d(kernel_size=layer.kernel_size, stride=layer.stride, padding = layer.padding)\n",
    "            model.add_module(name, avgpool)\n",
    "            i += 1\n",
    "            j = 0\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, style_losses, content_losses, input_image, num_iter):\n",
    "    optimizer = optim.LBFGS([input_image])\n",
    "    run = [0]\n",
    "    while run[0] <= num_iter:\n",
    "        def closure():\n",
    "            #input_image.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            model.forward(input_image)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "            for i, sl in enumerate(style_losses):\n",
    "                style_score += sl.loss\n",
    "            for i, cl in enumerate(content_losses):\n",
    "                content_score += cl.loss\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print (\"run {} : {}, {}\".format(run, style_score.data[0], content_score.data[0]))\n",
    "            loss = content_score + style_score\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chose your files here\n",
    "style_file = \"./neural_style/vangogh_starry_night.jpg\"\n",
    "content_file = \"./neural_style/Tuebingen_Neckarfront.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = image_loader(style_file).type(dtype)\n",
    "content = image_loader(content_file).type(dtype)\n",
    "# play around with the content and style weighta\n",
    "# to get the desired tradeoff between style and content preservation\n",
    "model, style_losses, content_losses = create_model(\n",
    "    content, style, cnn, content_weight=1.0, style_weight=10000 \n",
    ")\n",
    "input_im = image_loader(content_file).type(dtype)\n",
    "input_im = nn.Parameter(input_im.data)\n",
    "result = run_model(model, style_losses, content_losses, input_im, num_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(style.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(content.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(result.data.clamp_(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
